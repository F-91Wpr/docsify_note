## 部署 Maxwell

1. 启用 MySQL Binlog 服务

    ```shell
    sudo vim /etc/my.cnf
    ```
    
    添加如下配置：

    ```shell
    #数据库id
    server-id = 1
    #启动 binlog ，该参数的值会作为 binlog 的文件名前缀
    log-bin=mysql-bin
    # binlog 类型，maxwell 要求为 row 类型
    binlog_format=row
    #启用 binlog 的数据库，需根据实际情况作出修改
    binlog-do-db=gmall
    ```

    重启 MySQL 服务：

    ```shell
    sudo systemctl restart mysqld
    ```

2. 创建 Maxwell 所需数据库和用户
    
    ```shell
    msyql> CREATE DATABASE maxwell;

    #调整MySQL数据库密码级别
    mysql> set global validate_password_policy=0;
    mysql> set global validate_password_length=4;

    #创建Maxwell用户并赋予其必要权限
    mysql> CREATE USER 'maxwell'@'%' IDENTIFIED BY 'maxwell';
    mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'%';
    mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'%';
    ```

3. 修改 Maxwell 配置文件
   
    ```shell
    vim /opt/module/maxwell/config.properties
    ```

    ```shell
    #Maxwell数据发送目的地，可选配置有stdout|file|kafka|kinesis|pubsub|sqs|rabbitmq|redis
    producer=kafka
    #目标Kafka集群地址
    kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092
    #目标Kafka topic，可静态配置，例如:maxwell，也可动态配置，例如：%{database}_%{table}
    #kafka_topic=maxwell
    kafka_topic=topic_db

    #MySQL相关配置
    host=hadoop102
    user=maxwell
    password=maxwell
    jdbc_options=useSSL=false&serverTimezone=Asia/Shanghai

    # 补充
    client_id=maxwell_1
    ```

4. 测试

    ```shell
    #启动 Maxwell
    /opt/module/maxwell/bin/maxwell --config /opt/module/maxwell/config.properties --daemon

    #启动 kafka-console-consumer 用以观察
    kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic maxwell

    #模拟生成数据（dblog）
    java -jar /opt/module/dblog/gmall2020-mock-db-2021-11-14.jar
    ```

5. Maxwell 启停脚本

    ```shell
    #!/bin/bash

    MAXWELL_HOME=/opt/module/maxwell

    status_maxwell(){
        result=`ps -ef | grep com.zendesk.maxwell.Maxwell | grep -v grep | wc -l`
        return $result
    }


    start_maxwell(){
        status_maxwell
        if [[ $? -lt 1 ]]; then
            echo "启动Maxwell"
            $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon
        else
            echo "Maxwell正在运行"
        fi
    }


    stop_maxwell(){
        status_maxwell
        if [[ $? -gt 0 ]]; then
            echo "停止Maxwell"
            ps -ef | grep com.zendesk.maxwell.Maxwell | grep -v grep | awk '{print $2}' | xargs kill -9
        else
            echo "Maxwell未在运行"
        fi
    }


    case $1 in
        start )
            start_maxwell
        ;;
        stop )
            stop_maxwell
        ;;
        restart )
        stop_maxwell
        start_maxwell
        ;;
    esac

    ```

6. maxwell-bootstrap 功能：历史数据的全量同步

    ```shell
    maxwell-bootstrap --database gmall --table user_info --config /opt/module/maxwell/config.properties
    ```

    1. 第一条 type 为 bootstrap-start 和最后一条 type 为 bootstrap-complete 的数据，是 bootstrap 开始和结束的标志，不包含数据，中间的 type 为bootstrap-insert 的数据才包含数据。
    2. bootstrap 当次输出的所有记录的 ts 都相同，为 bootstrap 开始的时间。

7. 补充
   
    修改目标 kafka_topic 为 `topic_db`。
   
## 增量表 maxwell-bootstrap 全同步
1. 教学版 Maxwell
    此处为了模拟真实环境，对 Maxwell 源码进行了改动，增加了一个参数 mock_date ，该参数的作用就是指定 Maxwell 输出 JSON 字符串的 ts 时间戳的日期。

    修改 Maxwell 配置文件`config.properties`，增加`mock_date`参数，如下：

    ```shell
    log_level=info

    producer=kafka
    kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092

    #kafka topic配置
    kafka_topic=topic_db

    #注：该参数仅在maxwell教学版中存在，修改该参数后重启Maxwell才可生效
    mock_date=2020-06-14

    # mysql login info
    host=hadoop102
    user=maxwell
    password=maxwell
    jdbc_options=useSSL=false&serverTimezone=Asia/Shanghai
    ```

    重启后生效。

2. 增量表首日全同步脚本`mysql_to_kafka_inc_init.sh`
    
    ```shell
    vim mysql_to_kafka_inc_init.sh
    ```

    ```shell
    #!/bin/bash

    # 该脚本的作用是初始化所有的增量表，只需执行一次

    MAXWELL_HOME=/opt/module/maxwell

    import_data() {
        $MAXWELL_HOME/bin/maxwell-bootstrap --database gmall --table $1 --config $MAXWELL_HOME/config.properties
    }

    case $1 in
    "cart_info")
    import_data cart_info
    ;;
    "comment_info")
    import_data comment_info
    ;;
    "coupon_use")
    import_data coupon_use
    ;;
    "favor_info")
    import_data favor_info
    ;;
    "order_detail")
    import_data order_detail
    ;;
    "order_detail_activity")
    import_data order_detail_activity
    ;;
    "order_detail_coupon")
    import_data order_detail_coupon
    ;;
    "order_info")
    import_data order_info
    ;;
    "order_refund_info")
    import_data order_refund_info
    ;;
    "order_status_log")
    import_data order_status_log
    ;;
    "payment_info")
    import_data payment_info
    ;;
    "refund_payment")
    import_data refund_payment
    ;;
    "user_info")
    import_data user_info
    ;;
    "all")
    import_data cart_info
    import_data comment_info
    import_data coupon_use
    import_data favor_info
    import_data order_detail
    import_data order_detail_activity
    import_data order_detail_coupon
    import_data order_info
    import_data order_refund_info
    import_data order_status_log
    import_data payment_info
    import_data refund_payment
    import_data user_info
    ;;
    esac
    ```
    - 测试同步脚本

        1. 清理历史数据

            ```shell
            hadoop fs -ls /origin_data/gmall/db | grep _inc | awk '{print $8}' | xargs hadoop fs -rm -r -f
            ```

        2. 执行同步脚本

            ```shell
            mysql_to_kafka_inc_init.sh all
            ```

        3. 观察 hdfs 目录